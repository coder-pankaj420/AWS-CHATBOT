# RAG Chatbot with Google Gemini API

This project implements a Retrieval-Augmented Generation (RAG) chatbot application using FastAPI, LangChain, Pinecone vector database, and Google Gemini API. It ingests data from a PDF document, creates embeddings, stores and retrieves knowledge from Pinecone, and generates natural language answers using the Gemini language model.

---

## Features

- FastAPI backend with a `/chat` POST endpoint for question-answering
- PDF document ingestion and chunking using LangChain's PyPDFLoader and RecursiveCharacterTextSplitter
- Vector store using Pinecone for efficient semantic search
- Embeddings generated by `sentence-transformers/all-MiniLM-L6-v2` (Hugging Face)
- Natural language generation powered by Google's Gemini API (`gemini-1.5-flash-latest` free-tier model)
- Environment variable management with `python-dotenv` for secure API key handling
- Clear separation of concerns between the API server (`FastAPI_server.py`) and the AI logic (`RAG_chain.py`)

---

## Installation

1.  **Clone the repository:**
    ```
    git clone https://github.com/coder-pankaj420/AWS-CHATBOT.git
    cd AWS-CHATBOT
    ```

2.  **Create and activate the conda environment:**
    ```
    conda create -n chatbot_env python=3.10 -y
    conda activate chatbot_env
    ```

3.  **Install dependencies:**
    ```
    pip install -r requirements.txt
    ```

---

## Setup

1.  **Add your API keys** in a `.env` file in the project root (do not commit this file):
    ```
    GOOGLE_API_KEY=your_google_api_key_here
    PINECONE_API_KEY=your_pinecone_api_key_here
    ```

2.  **Place the PDF document** `AWS Description.pdf` in the project root directory.

---

## Usage

### 1. Build the Vector Store
Run this script once or whenever the PDF changes. This will populate your Pinecone database with knowledge from the PDF.

python VECTOR_DB.py



### 2. Run the FastAPI Server
Start the chatbot backend server. This will make the chatbot available via an API.

uvicorn FastAPI_server:app --reload


### 3. API Endpoints
-   **Interactive Docs:** `http://127.0.0.1:8000/docs`
-   **Chat Endpoint:** `POST /chat`
-   **Request Body:** `{"question": "Your question here"}`
-   **Response:** `{"answer": "The chatbot's answer"}`

---

## Key Files

-   **`VECTOR_DB.py`**: A script to handle data ingestion and create the embeddings in your vector store.
-   **`RAG_chain.py`**: Contains the core AI logic for the Retrieval-Augmented Generation chain.
-   **`FastAPI_server.py`**: The main FastAPI application that serves the chatbot API.
-   **`.env`**: Local environment variables for your secret API keys (not committed).

---

## Important Notes

-   Always keep your `.env` file secure and never commit API keys to source control.
-   To change the AI model, update the model name in `RAG_chain.py`:
    ```
    # Example model setting in RAG_chain.py
    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash-latest")
    ```
-   Ensure `load_dotenv()` is called at the top of any script that needs to access the API keys (`VECTOR_DB.py` and `RAG_chain.py`).

---

## Where Changes Are Needed

1.  **Add your API keys** to the `.env` file:
    ```
    GOOGLE_API_KEY=your_actual_google_api_key
    PINECONE_API_KEY=your_actual_pinecone_api_key
    ```

2.  **Ensure the model name** in `RAG_chain.py` matches the desired Gemini model:
    ```
    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash-latest")
    ```

3.  **Place your PDF file** (`AWS Description.pdf`) in the project root.

---

## Troubleshooting

-   If you encounter a `DefaultCredentialsError`, verify that `load_dotenv()` is called at the very top of `RAG_chain.py` and `VECTOR_DB.py` and that your `.env` file is correct.
-   If you get a `[Errno 10048]` "address already in use" error, it means a previous server process is still running. You will need to find and stop the process using that port.
-   If you get quota errors (`429`), consider upgrading your Google API key or using a different Gemini model with higher limits.

---

## License

MIT License
