
# RAG Chatbot with Google Gemini API

This project implements a Retrieval-Augmented Generation (RAG) chatbot application using FastAPI, LangChain, Pinecone vector database, and Google Gemini API. It ingests data from a PDF document, creates embeddings, stores and retrieves knowledge from Pinecone, and generates natural language answers using the Gemini language model.

---

## Features

- FastAPI backend with a `/chat` POST endpoint for question-answering
- PDF document ingestion and chunking using LangChain's PyPDFLoader and RecursiveCharacterTextSplitter
- Vector store using Pinecone for efficient semantic search
- Embeddings generated by `sentence-transformers/all-MiniLM-L6-v2` (Hugging Face)
- Natural language generation powered by Google's Gemini API (`gemini-1.5-flash-latest` free-tier model)
- Environment variable management with `python-dotenv` for secure API key handling

---

## Installation

1. Clone the repository:

```bash
git clone https://github.com/yourusername/your-repo-name.git
```

2. Create and activate the conda environment:

```bash
conda create -n chatbot_env python=3.10 -y
conda activate chatbot_env
```

3. Install dependencies:

```bash
pip install -r requirements.txt
```

---

## Setup

1. Add your API keys in a `.env` file in the project root (do not commit this file):

```
GOOGLE_API_KEY=your_google_api_key_here
PINECONE_API_KEY=your_pinecone_api_key_here
```

2. Place the PDF document `AWS Description.pdf` in the project root directory.

---

## Usage

### 1. Ingest the PDF and create embeddings

Run this script once or whenever the PDF changes:

```bash
python ingest.py
```

### 2. Run the FastAPI server

Start the chatbot backend server:

```bash
uvicorn main:app --reload
```

### 3. API Endpoints

Interactive Docs: http://127.0.0.1:8000/docs
Chat Endpoint: POST /chat
Request Body: {"question": "Your question here"}
Response: {"answer": "The chatbot's answer"}

---

## Key Files

- `ingest.py`: Data ingestion and embedding creation
- `main.py`: FastAPI backend serving the RAG chatbot
- `.env`: Environment variables for API keys (not committed)

---

## Important Notes

- Always keep your `.env` file secure and never commit API keys to source control.
- Update the model name in `main.py` if Google Gemini API models are updated:

```python
# Example model setting in main.py
llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash-latest")
```

- Use `load_dotenv()` at the very top of `main.py` to ensure environment variables are loaded before any library uses them.

---

## Where Changes Are Needed

1. **Add your API keys** to the `.env` file:

```
GOOGLE_API_KEY=your_actual_google_api_key
PINECONE_API_KEY=your_actual_pinecone_api_key
```

2. **Ensure the model name** in `main.py` matches the desired Gemini model:

```python
llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash-latest")
```

3. **Place your PDF file** (`AWS Description.pdf`) in the project root (next to `main.py` and `ingest.py`).


---

## Troubleshooting

- If you encounter a `DefaultCredentialsError`, verify that the `.env` file is named correctly and in the right location.
- If you get quota errors (`429`), consider upgrading your API key or using a different Gemini model with higher limits.

---

## License

MIT License
